{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Manager\n",
    "import pdb\n",
    "from numpy.lib.arraysetops import unique\n",
    "import tarfile\n",
    "import io\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def divide_chunks(l, chunk_size):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), chunk_size): \n",
    "        yield l[i:i + chunk_size]\n",
    "\n",
    "def create_subsets_of_searchlights(list_of_searchlights, num_searchlight_jobs):\n",
    "    job_size = math.floor(len(list_of_searchlights) / num_searchlight_jobs)\n",
    "    list_of_searchlight_subsets = list(divide_chunks(list_of_searchlights, chunk_size = job_size))\n",
    "    job_id_to_searchlight_subset = {}\n",
    "    j_index = 0\n",
    "    for job_id, searchlight_subset in enumerate(list_of_searchlight_subsets):\n",
    "        job_id_to_searchlight_subset[job_id] = searchlight_subset\n",
    "        j_index += 1\n",
    "    num_jobs_actual = j_index\n",
    "    if len(list_of_searchlight_subsets) != num_jobs_actual:\n",
    "        print(\"Error: len(list_of_searchlight_subsets) != num_jobs_actual \")\n",
    "        return\n",
    "    return job_id_to_searchlight_subset, num_jobs_actual\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dicts(dir, files_list):\n",
    "    dicts_list = []\n",
    "    index = 0\n",
    "    # get the dicts\n",
    "    print(\"get_dicts started\")\n",
    "    for file_name in files_list:\n",
    "        index += 1\n",
    "        if (index % 200) == 0:\n",
    "            print(index)\n",
    "        if \"searchlight_to_files_tuples\" in file_name:\n",
    "            # get the dict in this file\n",
    "            with open(dir   + \"assemble_light_to_tuples/\" +  file_name) as json_file:\n",
    "                d = json.load(json_file)\n",
    "                key = list(d.keys())[0]\n",
    "                new_dict = d[key]\n",
    "            dicts_list.append(new_dict)\n",
    "    print(\"get_dicts finished\")\n",
    "    return dicts_list\n",
    "\n",
    "def get_lights_list(dir, files_list):\n",
    "    the_list = []\n",
    "    print(\"get_lights_list started\")\n",
    "    index = 0\n",
    "    # get all the searchlights \n",
    "    for file_name in files_list:\n",
    "          \n",
    "        index += 1\n",
    "        if (index % 100) == 0:\n",
    "            print(index)\n",
    "        if \"list_of_searchlights\" in file_name:\n",
    "            # get the subset of searchlights in this particular file\n",
    "            with open(dir + \"assemble_lists_of_searchlights/\" +  file_name) as json_file:\n",
    "                d = json.load(json_file)\n",
    "                key = list(d.keys())[0]\n",
    "                new_searchlights = d[key]\n",
    "                for light_id in new_searchlights:\n",
    "                    the_list.append(light_id)\n",
    "    return the_list\n",
    "\n",
    "\n",
    "\n",
    "# # wrap things up\n",
    "# list_of_searchlights = get_lights_list(dir, os.listdir(dir + \"assemble_lists_of_searchlights/\"))\n",
    "# list_of_searchlight_to_files_tuples_dicts = get_dicts(dir, os.listdir(dir + \"assemble_light_to_tuples/\"))\n",
    "# print(\"len(list_of_searchlights): \", len(list_of_searchlights))\n",
    "# print(\"len(list_of_searchlight_to_files_tuples_dicts): \", len(list_of_searchlight_to_files_tuples_dicts))\n",
    "# f = open(dir + json_file_name, \"w\")\n",
    "# # break them up into jobs and then dump into json\n",
    "# job_id_to_searchlight_subset, num_jobs_actual = create_subsets_of_searchlights(list_of_searchlights,num_searchlight_jobs=num_searchlight_jobs)\n",
    "# json.dump({\"list_of_searchlight_to_files_tuples_dicts\": list_of_searchlight_to_files_tuples_dicts,\n",
    "#             \"job_id_to_searchlight_subset\": job_id_to_searchlight_subset,\n",
    "#             \"num_jobs_actual\": num_jobs_actual}, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_dicts started\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "10000\n",
      "10200\n",
      "10400\n",
      "10600\n",
      "10800\n",
      "11000\n",
      "11200\n",
      "11400\n",
      "11600\n",
      "11800\n",
      "12000\n",
      "12200\n",
      "12400\n",
      "12600\n",
      "12800\n",
      "13000\n",
      "13200\n",
      "13400\n",
      "13600\n",
      "13800\n",
      "14000\n",
      "14200\n",
      "14400\n",
      "14600\n",
      "14800\n",
      "15000\n",
      "15200\n",
      "15400\n",
      "15600\n",
      "15800\n",
      "16000\n",
      "16200\n",
      "16400\n",
      "16600\n",
      "16800\n",
      "17000\n",
      "17200\n",
      "17400\n",
      "17600\n",
      "17800\n",
      "18000\n",
      "18200\n",
      "18400\n",
      "18600\n",
      "18800\n",
      "19000\n",
      "19200\n",
      "19400\n",
      "19600\n",
      "19800\n",
      "20000\n",
      "20200\n",
      "20400\n",
      "20600\n",
      "20800\n",
      "21000\n",
      "21200\n",
      "21400\n",
      "21600\n",
      "21800\n",
      "22000\n",
      "22200\n",
      "22400\n",
      "22600\n",
      "22800\n",
      "23000\n",
      "23200\n",
      "23400\n",
      "23600\n",
      "23800\n",
      "24000\n",
      "24200\n",
      "24400\n",
      "24600\n",
      "24800\n",
      "25000\n",
      "25200\n",
      "25400\n",
      "25600\n",
      "25800\n",
      "26000\n",
      "26200\n",
      "26400\n",
      "26600\n",
      "26800\n",
      "27000\n",
      "27200\n",
      "27400\n",
      "27600\n",
      "27800\n",
      "28000\n",
      "28200\n",
      "28400\n",
      "28600\n",
      "28800\n",
      "29000\n",
      "29200\n",
      "29400\n",
      "29600\n",
      "29800\n",
      "30000\n",
      "30200\n",
      "30400\n",
      "30600\n",
      "30800\n",
      "31000\n",
      "31200\n",
      "31400\n",
      "31600\n",
      "31800\n",
      "32000\n",
      "32200\n",
      "32400\n",
      "32600\n",
      "32800\n",
      "33000\n",
      "33200\n",
      "33400\n",
      "33600\n",
      "33800\n",
      "34000\n",
      "34200\n",
      "34400\n",
      "34600\n",
      "34800\n",
      "35000\n",
      "35200\n",
      "35400\n",
      "35600\n",
      "35800\n",
      "36000\n",
      "36200\n",
      "36400\n",
      "36600\n",
      "36800\n",
      "37000\n",
      "37200\n",
      "37400\n",
      "37600\n",
      "37800\n",
      "38000\n",
      "38200\n",
      "38400\n",
      "38600\n",
      "38800\n",
      "39000\n",
      "39200\n",
      "39400\n",
      "39600\n",
      "39800\n",
      "40000\n",
      "40200\n",
      "40400\n",
      "40600\n",
      "40800\n",
      "41000\n",
      "41200\n",
      "41400\n",
      "41600\n",
      "41800\n",
      "42000\n",
      "42200\n",
      "42400\n",
      "42600\n",
      "42800\n",
      "43000\n",
      "43200\n",
      "43400\n",
      "43600\n",
      "43800\n",
      "44000\n",
      "44200\n",
      "44400\n",
      "44600\n",
      "44800\n",
      "45000\n",
      "45200\n",
      "45400\n",
      "45600\n",
      "45800\n",
      "46000\n",
      "46200\n",
      "46400\n",
      "46600\n",
      "46800\n",
      "47000\n",
      "47200\n",
      "47400\n",
      "47600\n",
      "47800\n",
      "48000\n",
      "48200\n",
      "48400\n",
      "48600\n",
      "48800\n",
      "49000\n",
      "49200\n",
      "49400\n",
      "49600\n",
      "49800\n",
      "50000\n",
      "50200\n",
      "50400\n",
      "50600\n",
      "50800\n",
      "51000\n",
      "51200\n",
      "51400\n",
      "51600\n",
      "51800\n",
      "52000\n",
      "52200\n",
      "52400\n",
      "52600\n",
      "52800\n",
      "53000\n",
      "53200\n"
     ]
    }
   ],
   "source": [
    "dir = \"/scratch/gpfs/rk1593/clustering_output/\" \n",
    "json_file_name = \"jobs_info_dict_manual_jupyter_without_tuples.json\"\n",
    "num_searchlight_jobs = 199\n",
    "# list_of_searchlights = get_lights_list(dir, os.listdir(dir + \"assemble_lists_of_searchlights/\"))\n",
    "dicts_list = get_dicts(dir, os.listdir(dir + \"assemble_light_to_tuples/\"))\n",
    "#f = open(dir + json_file_name, \"w\")\n",
    "# # break them up into jobs and then dump into json\n",
    "# job_id_to_searchlight_subset, num_jobs_actual = create_subsets_of_searchlights(list(unique_searchlights),num_searchlight_jobs=num_searchlight_jobs)\n",
    "#json.dump({\"list_of_searchlight_to_files_tuples_dicts\": dicts_list,\n",
    "            #\"unique_searchlights\": list(unique_searchlights),\n",
    "            #\"job_id_to_searchlight_subset\": job_id_to_searchlight_subset,\n",
    "            #\"num_jobs_actual\": num_jobs_actual}, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['72_91_45', '70_26_31', '41_72_66', '65_59_49']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dicts_list' (list)\n"
     ]
    }
   ],
   "source": [
    "%store dicts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_searchlights = set(list_of_searchlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'unique_searchlights' (set)\n"
     ]
    }
   ],
   "source": [
    "%store unique_searchlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%store -r unique_searchlights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r dicts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/scratch/gpfs/rk1593/clustering_output/\" \n",
    "json_file_name = \"jobs_info_dict_manual_jupyter_only_tuples_dicts.json\"\n",
    "f = open(\"/scratch/gpfs/rk1593/clustering_output/jobs_info_dict_manual_jupyter_only_tuples_dicts.json\", \"w\")\n",
    "json.dump({\"list_of_searchlight_to_files_tuples_dicts\": dicts_list}, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'job_id_to_searchlight_subset' (dict)\n"
     ]
    }
   ],
   "source": [
    "job_id_to_searchlight_subset, num_jobs_actual = create_subsets_of_searchlights(list(unique_searchlights),num_searchlight_jobs=num_searchlight_jobs)\n",
    "\n",
    "%store job_id_to_searchlight_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r job_id_to_searchlight_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-58-28a22f4def74>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-28a22f4def74>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    requires: searchlight_to_files_tuples for all searchlights, subset_list_of_searchlights are a list of the searchlights we want to include for this job\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_all_files_for_searchlight(list_of_searchlight_to_files_tuples_dicts, light_id):\n",
    "    files_list = []\n",
    "    # go through each dictionary of the X dictionaries\n",
    "    for searchlight_to_files_tuples_dicts in list_of_searchlight_to_files_tuples_dicts:\n",
    "        # if this light id is in this dictionary add all files to the files list\n",
    "        if light_id in searchlight_to_files_tuples_dicts:\n",
    "            for file in searchlight_to_files_tuples_dicts[light_id]:\n",
    "                files_list.append(file)\n",
    "    return files_list\n",
    "requires: searchlight_to_files_tuples for all searchlights, subset_list_of_searchlights are a list of the searchlights we want to include for this job\n",
    "# outputs a csv file for each searchlight\n",
    "def go_from_searchlight_files_dict_to_trinary_representation(light_to_template_to_pid_to_cond_to_lists,\n",
    "    light_to_template_to_compare_to_pid_to_tr_diffs,\n",
    "    light_to_template_to_compare_to_trTstats,\n",
    "    list_of_searchlight_to_files_tuples_dicts,subset_list_of_searchlights, \n",
    "    feedin_first_dict = False,\n",
    "    t_threshold = 0.2, just_the_mean = False, ignore_threshold = False, testing = False, \n",
    "            output_dir = \"/scratch/network/rk1593/\",  only_events2_3_and_4 = True):\n",
    "    cond_types = []\n",
    "    compares_list = [(0,1),(0,2), (0,3),(1,2),(1,3),(2,3)]\n",
    "    pdb.set_trace()\n",
    "    # go through each searchlight in the subset that we want in in this job\n",
    "    for counter_light, light_id in enumerate(subset_list_of_searchlights):\n",
    "        print(light_id)\n",
    "        files_this_light = get_all_files_for_searchlight(list_of_searchlight_to_files_tuples_dicts, light_id)\n",
    "        num_files_this_light = len(files_this_light)\n",
    "        if num_files_this_light != 480:\n",
    "            print(\"Error: searchlight \" + light_id + \" has \" + str(num_files_this_light) + \" files.\" )\n",
    "\n",
    "        if feedin_first_dict:\n",
    "            template_to_pid_to_cond_to_lists = light_to_template_to_pid_to_cond_to_lists[light_id]\n",
    "        else:\n",
    "            template_to_pid_to_cond_to_lists = {}\n",
    "            # go through each file in that searchlight, and note that the file has already\n",
    "            # been splitted before to extract the pid, template id and condition\n",
    "            i = 0\n",
    "            \n",
    "            for (file_name, pid, template_id, cond,tar_file_name, in_dir) in files_this_light:\n",
    "            \n",
    "                # change the directory to this in_dir to be able to open the tar file\n",
    "                os.chdir(in_dir)\n",
    "                # i += 1\n",
    "                #print(i)\n",
    "                # if we are at the first searchlight\n",
    "                if counter_light == 0:\n",
    "                    # then do the adding the cond_types list\n",
    "                    if cond not in cond_types:\n",
    "                        cond_types.append(cond)\n",
    "                tar = tarfile.open(tar_file_name)\n",
    "                in_text = tar.extractfile(file_name).read()\n",
    "                csv_file = io.StringIO(in_text.decode('ascii'))\n",
    "                random_replacer_for_nothing = str(1000000000)\n",
    "                csv_lines = [[y.replace(\" \", \"\") for y in x] for x in csv.reader(csv_file)]\n",
    "                if testing:\n",
    "                    csv_lines[0][0] = \"-1000\"\n",
    "             \n",
    "                \n",
    "                csv_lines = [[y if y != \"\" else random_replacer_for_nothing for y in x] for x in csv_lines]\n",
    "                #print(i)\n",
    "                try:\n",
    "                    new_arr = np.array(csv_lines).astype(\"float\")\n",
    "                    new_arr[new_arr == int(random_replacer_for_nothing)] = np.nan\n",
    "                except ValueError:\n",
    "                    pdb.set_trace()\n",
    "\n",
    "                # if in testing mode we need to crop out the first column and the first row \n",
    "                if testing:\n",
    "                    mean_list = np.nanmean(new_arr[:, 1:75], axis = 0).tolist()\n",
    "                else:\n",
    "                    mean_list = np.nanmean(new_arr).tolist()\n",
    "                if template_id not in template_to_pid_to_cond_to_lists:\n",
    "                    template_to_pid_to_cond_to_lists[template_id] = {}\n",
    "                if pid not in template_to_pid_to_cond_to_lists[template_id]:\n",
    "                    template_to_pid_to_cond_to_lists[template_id][pid] = {}\n",
    "                template_to_pid_to_cond_to_lists[template_id][pid][cond] = mean_list\n",
    "            light_to_template_to_pid_to_cond_to_lists[light_id] = template_to_pid_to_cond_to_lists\n",
    "        # now create the trinary representation\n",
    "        if not feedin_first_dict:\n",
    "            if len(cond_types) != 4:\n",
    "                print(\"Error: cond_types not 4\")\n",
    "                return\n",
    "        else:\n",
    "            cond_types = ['sameEv-sameSchema', 'sameEv-otherSchema',\n",
    "                                'otherEv-sameSchema',\n",
    "                                'otherEv-otherSchema']\n",
    "\n",
    "       \n",
    "        template_to_compare_to_pid_to_tr_diffs = {}\n",
    "        for template_id in template_to_pid_to_cond_to_lists:\n",
    "            if template_id not in template_to_compare_to_pid_to_tr_diffs:\n",
    "                template_to_compare_to_pid_to_tr_diffs[template_id] = {}\n",
    "            for pid in template_to_pid_to_cond_to_lists[template_id]:\n",
    "                for comparison_name in all_compare_names:\n",
    "                    \n",
    "                    compare1_name,compare2_name = comparison_name.split(\"_\")    \n",
    "                    if comparison_name not in template_to_compare_to_pid_to_tr_diffs[template_id]:\n",
    "                        template_to_compare_to_pid_to_tr_diffs[template_id][comparison_name] = {}\n",
    "                    compare1_list =  template_to_pid_to_cond_to_lists[template_id][pid][compare1_name]\n",
    "                    compare2_list = template_to_pid_to_cond_to_lists[template_id][pid][compare2_name]\n",
    "                    if len(compare1_list) != len(compare2_list):\n",
    "                        print(\"Error: for the same template and light, two paths have different number of tr's\")\n",
    "                        return\n",
    "                    tr_differences = [(compare1_list[i] - compare2_list[i]) for i in range(len(compare1_list))]\n",
    "                    if only_events2_3_and_4:\n",
    "                        tr_differences = tr_differences[event_2_start:event_5_start]\n",
    "                    template_to_compare_to_pid_to_tr_diffs[template_id][comparison_name][pid] = tr_differences\n",
    "        light_to_template_to_compare_to_pid_to_tr_diffs[light_id] = template_to_compare_to_pid_to_tr_diffs\n",
    "        # take the differences at each TR and do a ttest\n",
    "\n",
    "        \n",
    "        template_to_compare_to_trTstats = {}\n",
    "        for template_id in template_to_compare_to_pid_to_tr_diffs:\n",
    "            for compare_name in template_to_compare_to_pid_to_tr_diffs[template_id]:\n",
    "                # check that all participants have the same length of tr_diffs\n",
    "                # while also getting list of diffs at each tr\n",
    "                length_tr_list = []\n",
    "                tr_num_to_list_of_diffs = {}\n",
    "                for pid in template_to_compare_to_pid_to_tr_diffs[template_id][compare_name]:\n",
    "                    tr_diffs = template_to_compare_to_pid_to_tr_diffs[template_id][compare_name][pid]\n",
    "                    for index,diff in enumerate(tr_diffs):\n",
    "                        if index not in tr_num_to_list_of_diffs:\n",
    "                            tr_num_to_list_of_diffs[index] = []\n",
    "                        tr_num_to_list_of_diffs[index].append(diff)\n",
    "                    length_tr_list.append(len(tr_diffs))\n",
    "                length_tr_list = np.array(length_tr_list)\n",
    "                if not np.all(length_tr_list[0] == length_tr_list):\n",
    "                    pdb.set_trace()\n",
    "                    print(\"Error: not all the same length_tr_lists\")\n",
    "                    return\n",
    "                # now go through each tr index, and get a ttest\n",
    "                # make sure that we have 40 prticipants in each tr num\n",
    "                for tr_num in tr_num_to_list_of_diffs:\n",
    "                    if len(tr_num_to_list_of_diffs[tr_num]) != 40:\n",
    "                        pdb.set_trace()\n",
    "                        print(\"Error: missing participant\")\n",
    "                        return\n",
    "                \n",
    "                tr_Tstats = [get_t_stat_of_list(tr_num_to_list_of_diffs[tr_num], t_threshold, just_the_mean = just_the_mean, ignore_threshold = ignore_threshold) for tr_num in range(0,len(tr_num_to_list_of_diffs.keys()))]\n",
    "                if template_id not in template_to_compare_to_trTstats:\n",
    "                    template_to_compare_to_trTstats[template_id] = {}\n",
    "                template_to_compare_to_trTstats[template_id][compare_name] = tr_Tstats\n",
    "        light_to_template_to_compare_to_trTstats[light_id] = template_to_compare_to_trTstats\n",
    "        # for each searchlight get a feature list\n",
    "      \n",
    "        \n",
    "        features = []\n",
    "        for template_id in [2,3,4]:\n",
    "            for compare_name in all_compare_names:\n",
    "                for tr_stat in template_to_compare_to_trTstats[template_id][compare_name]:\n",
    "                    features.append(tr_stat)\n",
    "\n",
    "        features_np = np.array(features)\n",
    "        if testing:\n",
    "            save_path = output_dir +  \"/searchlights_testing/\" + light_id + \".csv\"\n",
    "        else:\n",
    "            save_path = output_dir +  \"/searchlights/\" + light_id + \".csv\"\n",
    "        np.savetxt(save_path, features_np, delimiter=\",\")\n",
    "    return light_to_template_to_pid_to_cond_to_lists, light_to_template_to_compare_to_pid_to_tr_diffs, light_to_template_to_compare_to_trTstats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-55-8d46930b0d63>\u001b[0m(24)\u001b[0;36mgo_from_searchlight_files_dict_to_trinary_representation\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     22 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     23 \u001b[0;31m    \u001b[0;31m# go through each searchlight in the subset that we want in in this job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 24 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mcounter_light\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlight_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_list_of_searchlights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlight_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0mfiles_this_light\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_files_for_searchlight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_searchlight_to_files_tuples_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlight_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "list_of_searchlight_to_files_tuples_dicts = dicts_list\n",
    "light_to_template_to_compare_to_pid_to_tr_diffs = {}\n",
    "light_to_template_to_compare_to_trTstats = {}\n",
    "light_to_template_to_pid_to_cond_to_lists = {}\n",
    "feedin_first_dict = False\n",
    "this_job_searchlights = job_id_to_searchlight_subset[1]\n",
    "t_threshold = 0.2\n",
    "just_the_mean = False\n",
    "ignore_threshold = False\n",
    "only_events2_3_and_4 = True\n",
    "testing = False\n",
    "output_dir = \"/scratch/gpfs/rk1593/clustering_output/\" \n",
    "light_to_template_to_pid_to_cond_to_lists, light_to_template_to_compare_to_pid_to_tr_diffs, light_to_template_to_compare_to_trTstats  = go_from_searchlight_files_dict_to_trinary_representation(light_to_template_to_pid_to_cond_to_lists = light_to_template_to_pid_to_cond_to_lists,\n",
    "                                light_to_template_to_compare_to_pid_to_tr_diffs = light_to_template_to_compare_to_pid_to_tr_diffs,\n",
    "                                light_to_template_to_compare_to_trTstats = light_to_template_to_compare_to_trTstats,\n",
    "                                list_of_searchlight_to_files_tuples_dicts = list_of_searchlight_to_files_tuples_dicts, \n",
    "                                feedin_first_dict = feedin_first_dict,\n",
    "                                subset_list_of_searchlights= this_job_searchlights, \n",
    "\n",
    "                                t_threshold = t_threshold, just_the_mean = just_the_mean, \n",
    "                                ignore_threshold = ignore_threshold, testing = testing, \n",
    "                                 output_dir = output_dir,  only_events2_3_and_4 = only_events2_3_and_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wedding_schema [~/.conda/envs/wedding_schema/]",
   "language": "python",
   "name": "conda_wedding_schema"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
